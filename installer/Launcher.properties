# suppress inspection "UnusedProperty" for whole file
#
# Sample properties file for JXTandem Launcher
#
#
# may be specified on the command line as well
#params=tandem.xml
#
# If using a cluster this is the hdfs directory    if unspecified
# will run locally
# use line below to run remotely
# remoteBaseDirectory=s3n://lordjoe/VentnerEcoli/
 #remoteBaseDirectory=File:///user/howdah/JXTandem/data/<LOCAL_DIRECTORY>
#use line below to run on the cluster
#remoteBaseDirectory=/user/howdah/JXTandem/data/<LOCAL_DIRECTORY>
remoteBaseDirectory=/user/slewis/clustering/<LOCAL_DIRECTORY>
#
#  hdfs job tracker
remoteHost=hadoop-master-03.ebi.ac.uk
#remoteHost=hadoop-master-01.ebi.ac.uk
#
#remoteHost=glados
#
#  port on the job tracker
#remotePort= 9000
#remotePort=8020
remotePort= 54310
 #
#  Actual job tracker with port - not the same as name node port
#remoteJobTracker= hadoop-master-02.ebi.ac.uk:9000
remoteJobTracker= hadoop-master-04.ebi.ac.uk:54311
#
#remoteJobTracker= glados:9001
#


# when there are multiple clusters running different versions of hadoop
# this is the 0.2 host and port
hadoop02Host=hadoop-master-03.ebi.ac.uk
hadoop02Port=54310
hadoop02remoteJobTracker=hadoop-master-04.ebi.ac.uk:54311

# when there are multiple clusters running different versions of hadoop
# this is the 0.2 host and port
hadoop10Host=hadoop-master-01.ebi.ac.uk
hadoop10Port=8020
hadoop10remoteJobTracker= hadoop-master-02.ebi.ac.uk:9000

#
#  user on the job tracker
remoteUser=slewis
#
#  encrypted password  on the job tracker alternative is plainTextRemotePassword
encryptedRemotePassword=aWnQ4DbdoSGivLcfi56hGKK8tx+LnqEYory3H4ueoRiivLcfi56hGA==
#
#  plain password  on the job tracker alternative is encryptedRemotePassword
#plainTextRemotePassword=secret

 #
# sets mapres.max.splt.size
#  a lower number forces more mappers in my splitters which is good
# the default is 64mb but for what we are doing smaller seems to be better
# when mappers do a lot of work - this is 16 megs
#maxSplitSize=16777216
maxSplitSize=3000000

#
# MaxReduceTasks tunes the cluster - increase this number for
# bigger clusters
maxReduceTasks = 300

#
#
# delete hadoop directories after the process is finished with them
# set fo false if debugging and the intermediate data wants to be examined
#deleteOutputDirectories=true
 #
 # compressed files take less space and are harder to read
# set false if you plan to manually read intermediate files
compressIntermediateFiles=true
 #
 # Maximum memory assigned to child tasks  in megabytes
 # maps to "mapred.child.java.opts", "-Xmx" + maxMamory + "m"
maxClusterMemory=2600




#
# This parameter gives a task longer to repost
# before returning  default value is 60000
DEFINE_mapred.task.timeout=1500000


#
# override specific Hadoop prepoerties - save writing specific code
DEFINE_io.sort.factor=100
DEFINE_io.sort.mb=600
# don't start reducers until most mappers done
#DEFINE_mapred.reduce.slowstart.completed.maps=0.5
#  number of map tasks for the database
DEFINE_org.systemsbiology.jxtandem.DesiredDatabaseMappers=64
#  number of map tasks for the spectra
DEFINE_org.systemsbiology.jxtandem.DesiredXMLInputMappers=7
