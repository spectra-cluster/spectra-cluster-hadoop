<?xml version="1.0"?>

<!--
    Overwrite hadoop configurations for major peak
-->

<configuration>

    <!-- The number of reducer tasks -->
    <property>
        <name>mapreduce.job.reduces</name>
        <value>300</value>
        <description>The number of reducers used for the job</description>
    </property>

    <!-- Memory allocation -->
    <property>
        <name>mapreduce.map.java.opts</name>
        <value>-Xmx1500m</value>
        <description>Memory assigned to each mapper JVM</description>
    </property>

    <property>
        <name>mapreduce.map.memory.mb</name>
        <value>1024</value>
        <description>Memory assigned to each mapper task in MB</description>
    </property>

    <property>
        <name>mapreduce.reduce.java.opts</name>
        <value>-Xmx2500m</value>
        <description>Memory assigned to each mapper JVM</description>
    </property>

    <property>
        <name>mapreduce.reduce.memory.mb</name>
        <value>2048</value>
        <description>Memory assigned to each mapper task in MB</description>
    </property>

    <!-- mapper compressions -->
    <property>
        <name>mapreduce.map.output.compress</name>
        <value>true</value>
        <description>Whether to compress mapper's output</description>
    </property>

    <property>
        <name>mapreduce.map.output.compress.codec</name>
        <value>org.apache.hadoop.io.compress.BZip2Codec</value>
        <description>Compress codec for mapper's output, using BZip since it supports splitting</description>
    </property>

    <property>
        <name>mapreduce.output.fileoutputformat.compress.type</name>
        <value>BLOCK</value>
        <description>Compress type for mapper or reducer output, change from RECORD to BLOCK to make it more efficient</description>
    </property>

    <!-- map-reduce task timeout -->
    <property>
        <name>mapreduce.task.timeout</name>
        <value>1500000</value>
        <description>The number of milliseconds before a task will be terminated if it neither reads an input, writes an output, nor updates its status string</description>
    </property>

    <!-- mapper buffer -->
    <property>
        <name>mapreduce.task.io.sort.mb</name>
        <value>600</value>
        <description>Each mapper task has a circular memory that it writes the output to</description>
    </property>

    <property>
        <name>mapreduce.task.io.sort.factor</name>
        <value>100</value>
        <description>This property controls the maximum number of streams to merge at once, the default is 10</description>
    </property>

    <!-- profiler -->
    <!-- not recommended for computational intensive tasks -->
    <property>
        <name>mapreduce.task.profile</name>
        <value>false</value>
        <description>Whether to enable the JVM build-in profiler</description>
    </property>

    <property>
        <name>mapreduce.task.profile.params</name>
        <value>-agentlib:hprof=cpu=samples,heap=sites,interval=20,depth=6,force=n,thread=n,verbose=n,file=%s</value>
        <description>Profiler configuration parameters</description>
    </property>

    <property>
        <name>mapreduce.task.profile.maps</name>
        <value>0</value>
        <description>the mapper tasks to be profiled</description>
    </property>

    <property>
        <name>mapreduce.task.profile.reduces</name>
        <value>0</value>
        <description>the reducer tasks to be profiled</description>
    </property>

</configuration>